{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Implementing an LSTM RNN Model\n",
    "------------------------\n",
    "Here we implement an LSTM model on all a data set of Shakespeare works.\n",
    "\n",
    "We start by loading the necessary libraries and resetting the default computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We start a computational graph session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, it is important to set the algorithm and data processing parameters.\n",
    "\n",
    "---------\n",
    "Parameter  :  Descriptions\n",
    " - min_word_freq: Only attempt to model words that appear at least 5 times.\n",
    " - rnn_size: size of our RNN (equal to the embedding size)\n",
    " - epochs: Number of epochs to cycle through the data\n",
    " - batch_size: How many examples to train on at once\n",
    " - learning_rate: The learning rate or the convergence paramter\n",
    " - training_seq_len: The length of the surrounding word group (e.g. 10 = 5 on each side)\n",
    " - embedding_size: Must be equal to the rnn_size\n",
    " - save_every: How often to save the model\n",
    " - eval_every: How often to evaluate the model\n",
    " - prime_texts: List of test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set RNN Parameters\n",
    "min_word_freq = 5 # Trim the less frequent words off\n",
    "rnn_size = 128 # RNN Model size, has to equal embedding size\n",
    "epochs = 10 # Number of epochs to cycle through data\n",
    "batch_size = 100 # Train on this many examples at once\n",
    "learning_rate = 0.001 # Learning rate\n",
    "training_seq_len = 50 # how long of a word group to consider \n",
    "embedding_size = rnn_size\n",
    "save_every = 500 # How often to save model checkpoints\n",
    "eval_every = 50 # How often to evaluate the test sentences\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']\n",
    "\n",
    "# Download/store Shakespeare data\n",
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "\n",
    "# Declare punctuation to remove, everything except hyphens and apostrophes\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])\n",
    "\n",
    "# Make Model Directory\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "\n",
    "# Make data directory\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Download the data if we don't have it saved already.  The data comes from the [Gutenberg Project](http://www.gutenberg.org])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare Data\n",
      "Not found, downloading Shakespeare texts from www.gutenberg.org\n",
      "Cleaning Text\n"
     ]
    }
   ],
   "source": [
    "print('Loading Shakespeare Data')\n",
    "# Check if file is downloaded.\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    # Get Shakespeare text\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    # Decode binary into string\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    # Drop first few descriptive paragraphs.\n",
    "    s_text = s_text[7675:]\n",
    "    # Remove newlines\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    \n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), encoding='utf-8', mode='w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    # If file has been saved, load from that file\n",
    "    with open(os.path.join(data_dir, data_file), encoding='utf-8', mode='r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')\n",
    "\n",
    "# Clean text\n",
    "print('Cleaning Text')\n",
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text ).strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define a function to build a word processing dictionary (word -> ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build word vocabulary function\n",
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    # limit word counts to those more frequent than cutoff\n",
    "    word_counts = {key:val for key, val in word_counts.items() if val>min_word_freq}\n",
    "    # Create vocab --> index mapping\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    # Add unknown key --> 0 index\n",
    "    vocab_to_ix_dict['unknown']=0\n",
    "    # Create index --> vocab mapping\n",
    "    ix_to_vocab_dict = {val:key for key,val in vocab_to_ix_dict.items()}\n",
    "    \n",
    "    return(ix_to_vocab_dict, vocab_to_ix_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we can build the index-vocabulary from the Shakespeare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Shakespeare Vocab\n",
      "Vocabulary Length = 8009\n"
     ]
    }
   ],
   "source": [
    "# Build Shakespeare vocabulary\n",
    "print('Building Shakespeare Vocab')\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1\n",
    "print('Vocabulary Length = {}'.format(vocab_size))\n",
    "# Sanity Check\n",
    "assert(len(ix2vocab) == len(vocab2ix))\n",
    "\n",
    "# Convert text to word vectors\n",
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We define the LSTM model.  The methods of interest are the `__init__()` method, which defines all the model variables and operations, and the `sample()` method which takes in a sample word and loops through to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define LSTM RNN Model\n",
    "class LSTM_Model():\n",
    "    def __init__(self, rnn_size, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # Define Embedding\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.rnn_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # If we are inferring (generating text), we add a 'loop' function\n",
    "        # Define how to get the i+1 th input from the i th output\n",
    "        def inferred_loop(prev, count):\n",
    "            # Apply hidden layer\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # Get the index of the output (also don't run the gradient)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # Get embedded vector\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return(output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return(out_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to use the same model (with the same trained variables), we need to share the variable scope between the trained model and the test model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define LSTM Model\n",
    "lstm_model = LSTM_Model(rnn_size, batch_size, learning_rate,\n",
    "                        training_seq_len, vocab_size)\n",
    "\n",
    "# Tell TensorFlow we are reusing the scope for the testing\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(rnn_size, batch_size, learning_rate,\n",
    "                                 training_seq_len, vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We need to save the model, so we create a model saving operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create model saver\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's calculate how many batches are needed for each epoch and split up the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create batches for each epoch\n",
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n",
    "# Split up text indices into subarrays, of equal size\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "# Reshape each split into [batch_size, training_seq_len]\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Initialize all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch #1 of 10.\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 9.88\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182, Loss: 9.11\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182, Loss: 8.63\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182, Loss: 8.31\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182, Loss: 8.06\n",
      "thou art more thou thou thou\n",
      "to be or not to the\n",
      "wherefore art thou thou thou thou\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle word indices\n",
    "    random.shuffle(batches)\n",
    "    # Create targets from shuffled batches\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    # Run a through one epoch\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch+1, epochs))\n",
    "    # Reset initial LSTM state every epoch\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        # Print status every 10 gens\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "        # Save the model and the vocab\n",
    "        if iteration_count % save_every == 0:\n",
    "            # Save model\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step = iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            # Save vocabulary\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "                \n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here is a plot of the training loss across the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot loss over time\n",
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
